{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvWx6aeJm2CU"
      },
      "source": [
        "# The Forward-Forward Algorithm\n",
        "\n",
        "Original paper: https://www.cs.toronto.edu/~hinton/FFA13.pdf\n",
        "\n",
        "![ViT](./media/backprop_vs_ff.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2wV5ULym2CV",
        "outputId": "1a4d9f55-701f-4945-a62a-c1394d4a43b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# !pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2WwCX9RAm2CW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "from dataset_utils import MNISTLoader, TrainingDatasetFF, CIFAR10Loader, EMNISTLoader, CIFAR100Loader\n",
        "from models import FFMultiLayerPerceptron, MultiLayerPerceptron, FFCNN, BPCNN\n",
        "from tools import base_loss, generate_positive_negative_samples_overlay\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, Normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BmuBAxR5m2CW"
      },
      "outputs": [],
      "source": [
        "## -- Set some variables\n",
        "PATH_DOWNLOAD = './tmp'\n",
        "torch.manual_seed(0)\n",
        "train_batch_size = 1024\n",
        "test_batch_size = 1024\n",
        "pos_gen_fn = generate_positive_negative_samples_overlay # which function to use to generate pos neg examples\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU7oR0F1m2CW"
      },
      "source": [
        "# 1.0 Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59CJE7Scm2CW",
        "outputId": "91339621-a8c0-4b6e-f7b2-073e21082fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./tmp/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102174720.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./tmp/MNIST/raw/train-images-idx3-ubyte.gz to ./tmp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./tmp/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 69220396.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./tmp/MNIST/raw/train-labels-idx1-ubyte.gz to ./tmp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./tmp/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27867331.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./tmp/MNIST/raw/t10k-images-idx3-ubyte.gz to ./tmp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./tmp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9119448.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./tmp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./tmp/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "download_folder = Path(PATH_DOWNLOAD).mkdir(parents=True, exist_ok=True)\n",
        "pick_dataset = \"MNIST\"\n",
        "\n",
        "if (pick_dataset == \"MNIST\"):\n",
        "  transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.1307,), (0.3081,)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "  data_loader = MNISTLoader(train_transform=transform,\n",
        "                            test_transform=transform)\n",
        "  hidden_dimensions = [784, 500, 500, 500] # first is input size\n",
        "  num_classes = 10\n",
        "  kernel_size = 3\n",
        "\n",
        "elif (pick_dataset == \"CIFAR10\"):\n",
        "  transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "  data_loader = CIFAR10Loader(train_transform=transform,\n",
        "                            test_transform=transform)\n",
        "  hidden_dimensions = [3072, 500, 500, 500]\n",
        "  num_classes = 10\n",
        "  kernel_size = 3\n",
        "\n",
        "elif (pick_dataset == \"CIFAR100\"):\n",
        "  transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "  data_loader = CIFAR100Loader(train_transform=transform,\n",
        "                            test_transform=transform)\n",
        "  hidden_dimensions = [3072, 500, 500, 500]\n",
        "  num_classes = 100\n",
        "  kernel_size = 3\n",
        "\n",
        "\n",
        "elif (pick_dataset == \"EMNIST\"):\n",
        "  transform = Compose([\n",
        "      ToTensor(),\n",
        "      Normalize((0.5,), (0.5,)),\n",
        "      Lambda(lambda x: torch.flatten(x))])\n",
        "  data_loader = EMNISTLoader(train_transform=transform,\n",
        "                            test_transform=transform)\n",
        "  hidden_dimensions = [784, 500, 500]\n",
        "  num_classes = 62\n",
        "  kernel_size = 3\n",
        "\n",
        "\n",
        "data_loader.download_dataset()\n",
        "train_loader = data_loader.get_train_loader(train_batch_size)\n",
        "test_loader = data_loader.get_test_loader(test_batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BxLIIXZQm2CX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "4960a596-1514-48cf-fd76-7f0c56a7563c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6e4dfcff34ac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# it takes 10s to prepare all training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_loader_ff = torch.utils.data.DataLoader(TrainingDatasetFF(pos_gen_fn(X.to(device),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                            Y.to(device), False, num_classes)\n\u001b[1;32m      4\u001b[0m                                                                 for X, Y in train_loader),\n\u001b[1;32m      5\u001b[0m                                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_generator)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             self.dataset = [\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mX_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_neg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             self.dataset = [\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mX_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_neg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-6e4dfcff34ac>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# it takes 10s to prepare all training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_loader_ff = torch.utils.data.DataLoader(TrainingDatasetFF(pos_gen_fn(X.to(device),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                            Y.to(device), False, num_classes)\n\u001b[1;32m      4\u001b[0m                                                                 for X, Y in train_loader),\n\u001b[1;32m      5\u001b[0m                                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mclass\u001b[0m \u001b[0mArrayData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# see RawEncode.c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# it takes 10s to prepare all training dataset\n",
        "train_loader_ff = torch.utils.data.DataLoader(TrainingDatasetFF(pos_gen_fn(X.to(device),\n",
        "                                                                           Y.to(device), False, num_classes)\n",
        "                                                                for X, Y in train_loader),\n",
        "                                              batch_size=train_loader.batch_size, shuffle=True\n",
        "                                              )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd4yv4U1m2CX"
      },
      "source": [
        "# 2.0 Create Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYAtQzshm2CX"
      },
      "outputs": [],
      "source": [
        "## -- Set some variables\n",
        "activation = torch.nn.ReLU()\n",
        "#activation = torch.nn.Sigmoid()\n",
        "#activation = torch.nn.LeakyReLU()\n",
        "#activation = torch.nn.Softmax(dim=1)\n",
        "#activation = torch.nn.LogSoftmax(dim=1)\n",
        "layer_optim_learning_rate = 0.09\n",
        "optimizer = torch.optim.Adam\n",
        "threshold = 9.0\n",
        "loss = base_loss \n",
        "method = \"MSE\"\n",
        "model_arch = \"MLP\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmPhhMN9m2CX"
      },
      "outputs": [],
      "source": [
        "if model_arch == \"MLP\":\n",
        "  model = FFMultiLayerPerceptron(hidden_dimensions, \n",
        "                                    activation,\n",
        "                                    optimizer,\n",
        "                                    layer_optim_learning_rate,\n",
        "                                    threshold,\n",
        "                                    loss, method).to(device)\n",
        "elif model_arch == \"CNN\":\n",
        "  model = FFCNN(hidden_dimensions, activation, optimizer, \n",
        "                layer_optim_learning_rate,threshold, loss, \n",
        "                method, num_classes, kernel_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaLFw1379D_o"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return num_params\n",
        "\n",
        "count_parameters(model)\n",
        "# for layer in mlp_model.layers:\n",
        "#     # Count the parameters\n",
        "#     num_params = count_parameters(layer)\n",
        "#     print(\"Number of parameters:\", num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwTorfHdLx43"
      },
      "outputs": [],
      "source": [
        "def deepLIFT(model, x, baseline):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass with the actual input\n",
        "    output_actual = model(x)\n",
        "\n",
        "    # Forward pass with the baseline input\n",
        "    output_baseline = model(baseline)\n",
        "\n",
        "    # Compute the differences in activations\n",
        "    delta = output_actual - output_baseline\n",
        "\n",
        "    # Perform backward pass to compute importance scores\n",
        "    model.zero_grad()\n",
        "    delta.backward(torch.ones_like(delta))\n",
        "\n",
        "    # Retrieve the gradients from each input\n",
        "    importance_scores = x.grad\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "def visualize_importance_scores(image, importance_scores):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "    # Display the original image\n",
        "    ax[0].imshow(image, cmap='gray')\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # Overlay the importance scores on the image\n",
        "    ax[1].imshow(image, cmap='gray')\n",
        "    ax[1].imshow(importance_scores, cmap='hot', alpha=0.6)\n",
        "    ax[1].set_title('Importance Scores Overlay')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def normalize_scores(scores):\n",
        "    mean_baseline = torch.mean(train_loader, dim=0)\n",
        "    centered_scores = scores - mean_baseline\n",
        "    normalized_scores = centered_scores / centered_scores.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqM1nhdm2CY"
      },
      "source": [
        "## 3.0 Train Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEOxz0QRm2CY"
      },
      "outputs": [],
      "source": [
        "## -- Set some variables\n",
        "n_epochs = 60\n",
        "print_every_10_epochs = True\n",
        "\n",
        "# choose one of the following training procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTFPk26qm2CY"
      },
      "source": [
        "## 3.1 Train all layers at the same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh7-dWVXm2CY"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm(range(n_epochs)):\n",
        "    for X_pos, Y_neg in train_loader_ff:\n",
        "        layer_losses = model.train_batch(X_pos, Y_neg, before=False, method=method)\n",
        "        print(\", \".join(map(lambda i, l: 'Layer {}: {}'.format(i, l),list(range(len(layer_losses))) ,layer_losses)), end='\\r')\n",
        "\n",
        "    if epoch % 10 == 0 and print_every_10_epochs == True:\n",
        "      print(\"Epoch:\", epoch)\n",
        "      acc = 0\n",
        "      for X_train, Y_train in tqdm(train_loader, total=len(train_loader)):\n",
        "          X_train = X_train.to(device)\n",
        "          Y_train = Y_train.to(device)\n",
        "\n",
        "          acc += (model.predict_accomulate_goodness(X_train, pos_gen_fn, n_class=num_classes, method=method).eq(Y_train).sum())\n",
        "\n",
        "      train_accuracy = acc / float(len(train_loader.dataset))\n",
        "      train_error = 1 - train_accuracy\n",
        "\n",
        "\n",
        "      print(\"Overall Train Accuracy: {:.4%}\".format(train_accuracy))\n",
        "      print(\"Overall Train Error: {:.4%}\".format(train_error))\n",
        "      acc = 0\n",
        "\n",
        "      for X_test, Y_test in tqdm(test_loader, total=len(test_loader)):\n",
        "          X_test = X_test.to(device)\n",
        "          Y_test = Y_test.to(device)\n",
        "\n",
        "          acc += (model.predict_accomulate_goodness(X_test,\n",
        "                  pos_gen_fn, n_class=num_classes, method=method).eq(Y_test).sum())\n",
        "\n",
        "      print(f\"Accuracy: {acc/float(len(data_loader.test_set)):.4%}\")\n",
        "      print(f\"Test error: {1 - acc/float(len(data_loader.test_set)):.4%}\")\n",
        "\n",
        "\n",
        "        # Example usage\n",
        "        # image, label = next(iter(train_loader_ff))\n",
        "        # image = image.to(device)\n",
        "        # image = image.squeeze().cpu().numpy()\n",
        "        # importance_scores = deepLIFT(mlp_model, image, torch.cat([X_pos for X_pos, _ in iter(train_loader_ff)]).mean())  # Compute importance scores\n",
        "        # normalized_scores = normalize_scores(importance_scores)  # Normalize importance scores\n",
        "        # visualize_importance_scores(image, normalized_scores)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYrUiLHZx5wV"
      },
      "outputs": [],
      "source": [
        "# Calculate train accuracy after each epoch\n",
        "acc = 0\n",
        "for X_train, Y_train in tqdm(train_loader, total=len(train_loader)):\n",
        "    X_train = X_train.to(device)\n",
        "    Y_train = Y_train.to(device)\n",
        "\n",
        "    acc += (model.predict_accomulate_goodness(X_train, pos_gen_fn, n_class=num_classes, method=method).eq(Y_train).sum())\n",
        "\n",
        "train_accuracy = acc / float(len(train_loader.dataset))\n",
        "train_error = 1 - train_accuracy\n",
        "\n",
        "\n",
        "print(\"Overall Train Accuracy: {:.4%}\".format(train_accuracy))\n",
        "print(\"Overall Train Error: {:.4%}\".format(train_error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmwSN3Dm2CY"
      },
      "source": [
        "## 3.2 Train one layer at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICweFOKmm2CY"
      },
      "outputs": [],
      "source": [
        "#mlp_model.train_batch_progressive(n_epochs, train_loader_ff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbM5KrwCm2CY"
      },
      "source": [
        "# 4.0 Test the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_l7UANam2CY"
      },
      "outputs": [],
      "source": [
        "acc = 0\n",
        "\n",
        "for X_test, Y_test in tqdm(test_loader, total=len(test_loader)):\n",
        "    X_test = X_test.to(device)\n",
        "    Y_test = Y_test.to(device)\n",
        "\n",
        "    acc += (model.predict_accomulate_goodness(X_test,\n",
        "            pos_gen_fn, n_class=num_classes, method=method).eq(Y_test).sum())\n",
        "\n",
        "print(f\"Accuracy: {acc/float(len(data_loader.test_set)):.4%}\")\n",
        "print(f\"Test error: {1 - acc/float(len(data_loader.test_set)):.4%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_rqOI91m2CZ"
      },
      "source": [
        "# 5.0 Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lVGSOQKm2CZ"
      },
      "outputs": [],
      "source": [
        "## -- Set some variables\n",
        "n_epochs= 60\n",
        "# if pick_dataset == \"MNIST\":\n",
        "#   hidden_dimensions = [784, 500, 500, 10] # first is input size\n",
        "# elif pick_dataset == \"CIFAR10\":\n",
        "#   hidden_dimensions = [3072, 500, 500, 10]\n",
        "#activation = torch.nn.ReLU()\n",
        "optimizer = torch.optim.Adam\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sce6UpQbm2CZ"
      },
      "outputs": [],
      "source": [
        "if model_arch == \"MLP\":\n",
        "  backprop_model = MultiLayerPerceptron(hidden_dimensions, activation).to(device)\n",
        "  hidden_dimensions.append(num_classes)\n",
        "elif model_arch == \"CNN\":\n",
        "  backprop_model = BPCNN(hidden_dimensions, activation, num_classes, kernel_size).to(device)\n",
        "\n",
        "optimizer = optimizer(backprop_model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_lVlp1J9Ims"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return num_params\n",
        "\n",
        "count_parameters(backprop_model)\n",
        "# for layer in mlp_model.layers:\n",
        "#     # Count the parameters\n",
        "#     num_params = count_parameters(layer)\n",
        "#     print(\"Number of parameters:\", num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9lp17gXm2CZ"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    for i, (X_train, Y_train) in enumerate(train_loader):\n",
        "        X_train = X_train.to(device)\n",
        "        Y_train = Y_train.to(device)\n",
        "\n",
        "        Y_pred = backprop_model(X_train)\n",
        "\n",
        "        loss = loss_fn(Y_pred, Y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Loss: {loss}\", end='\\r')\n",
        "      \n",
        "    if epoch % 10 == 0 and print_every_10_epochs == True:\n",
        "      acc = 0\n",
        "      for X_train, Y_train in tqdm(train_loader, total=len(train_loader)):\n",
        "          X_train = X_train.to(device)\n",
        "          Y_train = Y_train.to(device)\n",
        "\n",
        "          acc += (torch.softmax(backprop_model(X_train), 1).argmax(1).eq(Y_train).sum())\n",
        "\n",
        "      print(\"Epoch: \", epoch)\n",
        "      print(f\"Accuracy: {acc/float(len(data_loader.train_set)):.4%}\")\n",
        "      print(f\"Train error: {1 - acc/float(len(data_loader.train_set)):.4%}\")\n",
        "\n",
        "      acc = 0\n",
        "      for X_test, Y_test in tqdm(test_loader, total=len(test_loader)):\n",
        "          X_test = X_test.to(device)\n",
        "          Y_test = Y_test.to(device)\n",
        "\n",
        "          acc += (torch.softmax(backprop_model(X_test), 1).argmax(1).eq(Y_test).sum())\n",
        "\n",
        "      print(f\"Accuracy: {acc/float(len(data_loader.test_set)):.4%}\")\n",
        "      print(f\"Test error: {1 - acc/float(len(data_loader.test_set)):.4%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9CkDIQIzw5f"
      },
      "outputs": [],
      "source": [
        "# Train accuracy\n",
        "acc = 0\n",
        "for X_train, Y_train in tqdm(train_loader, total=len(train_loader)):\n",
        "    X_train = X_train.to(device)\n",
        "    Y_train = Y_train.to(device)\n",
        "\n",
        "    acc += (torch.softmax(backprop_model(X_train), 1).argmax(1).eq(Y_train).sum())\n",
        "\n",
        "print(f\"Accuracy: {acc/float(len(data_loader.train_set)):.4%}\")\n",
        "print(f\"Test error: {1 - acc/float(len(data_loader.train_set)):.4%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i9PvSrRm2CZ"
      },
      "outputs": [],
      "source": [
        "# Test accuracy\n",
        "acc = 0\n",
        "for X_test, Y_test in tqdm(test_loader, total=len(test_loader)):\n",
        "    X_test = X_test.to(device)\n",
        "    Y_test = Y_test.to(device)\n",
        "\n",
        "    acc += (torch.softmax(backprop_model(X_test), 1).argmax(1).eq(Y_test).sum())\n",
        "\n",
        "print(f\"Accuracy: {acc/float(len(data_loader.test_set)):.4%}\")\n",
        "print(f\"Test error: {1 - acc/float(len(data_loader.test_set)):.4%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "6905339ec9834455f3eeaf833f5d6a2573f0df69b633997954458b6d6617aa92"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}